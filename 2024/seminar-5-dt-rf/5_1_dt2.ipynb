{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Деревья решений для классификации (продолжение)\n",
    "\n",
    "На прошлом занятии мы разобрали идею Деревьев решений:\n",
    "\n",
    "![DecisionTree](tree1.png)\n",
    "\n",
    "\n",
    "Давайте теперь разберемся **как происходит разделения в каждом узле** то есть как проходит этап **обучения модели**. Есть как минимум две причины в этом разобраться : во-первых это позволит нам решать задачи классификации на 3 и более классов, во-вторых это даст нам возможность считать *важность* признаков в обученной модели.\n",
    "\n",
    "Для начала посмотрим какие бывают деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "Дерево решений вообще говоря **не обязано быть бинарным**, на практике однако используются именно бинарные деревья, поскольку для любоого не бинарного дерева решений **можно построить бинарное** (при этом увеличится глубина дерева).\n",
    "\n",
    "### 1. Деревья решений использую простой одномерный предикат для разделения объектов\n",
    "\n",
    "Имеется ввиду что в каждом узле разделение объектов (и создание двух новых узлов) происходит **по 1 (одному)** признаку: \n",
    "\n",
    "*Все объекты со значением некоторого признака меньше трешхолда отправляются в один узел, а больше - в другой:*\n",
    "\n",
    "$$\n",
    "[x_j < t]\n",
    "$$\n",
    "\n",
    "Вообще говоря это совсем не обязательно, например в каждом отдельном узле можно строить любую модель (например логистическую регрессию или KNN), рассматривая сразу несколько признаков.\n",
    "\n",
    "### 2. Оценка качества \n",
    "\n",
    "Мы говорили про простой функционал качества разбиения (**выбора трешхолда**): количество ошибок (1-accuracy). \n",
    "На практике используются два критерия: Gini's impurity index и Information gain.\n",
    "\n",
    "**Индекс Джини**\n",
    "$$\n",
    "I_{Gini} = 1 - \\sum_i^K p_i^2 \n",
    "$$\n",
    "\n",
    "где $K$ - количество классов, a $p_i = \\frac{|n_i|}{n}$ - доля представителей $i$ - ого класса в данном узле\n",
    "\n",
    "\n",
    "**Энтропия**\n",
    "\n",
    "$$\n",
    "H(p) = - \\sum_i^K p_i\\log(p_i)\n",
    "$$\n",
    "\n",
    "**Информационный критерий**\n",
    "$$\n",
    "IG(p) = H(\\text{parent}) - H(\\text{child})\n",
    "$$\n",
    "\n",
    "\n",
    "#### Разделение производится по тому трешхолду и тому признаку по которому взвешенное среднее функционала качества в узлах потомках наименьшее.\n",
    "\n",
    "\n",
    "### 3. Критерий остановки\n",
    "\n",
    "Мы с вами говорили о таких параметрах Решающего дерева как минимальное число объектов в листе,\n",
    "и минимальное число объектов в узле, для того чтобы он был разделен на два. Еще один критерий - \n",
    "глубина дерева. Возможны и другие.\n",
    "\n",
    "* Ограничение числа объектов в листе\n",
    "* Ограничение числа объектов в узле, для того чтобы он был разделен\n",
    "* Ограничение глубины дерева\n",
    "* Ограничение минимального прироста Энтропии или Информационного критерия при разделении\n",
    "* Остановка в случае если все объекты в листе принадлежат к одному классу\n",
    "\n",
    "На прошлой лекции мы обсуждали технику которая называется **Прунинг** (pruning) это альтернатива Критериям остановки, когда сначала строится переобученное дерево, а затем она каким то образом упрощается. На практике по ряду причин чаще используются критерии остановки, а не прунинг.\n",
    "\n",
    "Подробнее см. https://github.com/esokolov/ml-course-hse/blob/master/2018-fall/lecture-notes/lecture07-trees.pdf\n",
    "\n",
    "Оссобенности разбиения непрерывных признаков\n",
    "* http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/\n",
    "* http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Оценка качества разделения в узле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y_current):\n",
    "    \n",
    "    n = y_current.shape[0]\n",
    "    val, count = np.unique(y_current, return_counts=True)\n",
    "    gini = 1 - ((count/n)**2).sum()\n",
    "        \n",
    "    return gini\n",
    "\n",
    "def entropy(y_current):\n",
    "    \n",
    "    gini = 1\n",
    "    n = y_current.shape[0]\n",
    "    val, count = np.unique(y_current, return_counts=True)\n",
    "    p = count/n\n",
    "    igain = p.dot(np.log(p))\n",
    "    \n",
    "    return igain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "Y_example = np.zeros((100,100))\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(i, 100):\n",
    "        Y_example[i, j] = 1\n",
    "        \n",
    "gini = [gini_impurity(y) for y in Y_example]\n",
    "ig = [-entropy(y) for y in Y_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.plot(np.linspace(0,1,100), gini, label='Index Gini');\n",
    "plt.plot(np.linspace(0,1,100), ig, label ='Entropy');\n",
    "plt.legend()\n",
    "plt.xlabel('Доля примеров\\n положительного класса')\n",
    "plt.ylabel('Значение оптимизируемого\\n функционала');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Пример работы Решающего дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Индекс Джини** и **Информационный критерий** это меры сбалансированности вектора (насколько значения объектов в наборе однородны). Максимальная неоднородность когда объектов разных классов поровну. Максимальная однородность когда в наборе объекты одного класса. \n",
    "\n",
    "Разбивая множество объектов на два подмножества, мы стремимся уменьшить неоднородность в каждом подмножестве.\n",
    "Посмотрем на примере Ирисов Фишера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ирисы Фишера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "model = DecisionTreeClassifier()\n",
    "model = model.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['sepal length', 'sepal width',  'petal length',  'petal width']\n",
    "target_names = ['setosa', 'versicolor', 'virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model.decision_path(iris.data).todense())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model.decision_path(iris.data).todense())[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tree_.node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цифры. Интерпретируемость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(n_class=2, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    ax.imshow(X[i].reshape(8,8), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model.decision_path(X).todense())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.feature_importances_.reshape(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(model, out_file='tree.dot', filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sudo apt-get install graphviz\n",
    "\n",
    "# !dot -Tpng 'tree.dot' -o 'tree.png'\n",
    "\n",
    "# ![Iris_tree](tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model.decision_path(X).todense())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Решающие деревья легко обобщаются на задачу многоклассовой классификации\n",
    "\n",
    "### Пример с рукописными цифрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(n_class=10, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3,3,i+1)\n",
    "    ax.imshow(X[i].reshape(8,8), cmap='gray')\n",
    "    ax.set_title(y[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.feature_importances_.reshape(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вопрос: откуда мы получаем feature importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Пример на котором дерево решений строит очень сложную разделяющую кривую\n",
    "\n",
    "Пример взят отсюда https://habr.com/ru/company/ods/blog/322534/#slozhnyy-sluchay-dlya-derevev-resheniy .\n",
    "\n",
    "Как мы помним Деревья используют одномерный предикат для разделени множества объектов.\n",
    "Это значит что если данные плохо разделимы по **каждому** (индивидуальному) признаку по отдельности, результирующее решающее правило может оказаться очень сложным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_linearly_separable_data(n=500, x1_min=0, x1_max=30, x2_min=0, x2_max=30):\n",
    "    data, target = [], []\n",
    "    for i in range(n):\n",
    "        x1, x2 = np.random.randint(x1_min, x1_max), np.random.randint(x2_min, x2_max)\n",
    "\n",
    "        if np.abs(x1 - x2) > 0.5:\n",
    "            data.append([x1, x2])\n",
    "            target.append(np.sign(x1 - x2))\n",
    "    return np.array(data), np.array(target)\n",
    "\n",
    "X, y = form_linearly_separable_data()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим как данные выглядит в проекции на 1 ось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('Проекция на ось $X_0$')\n",
    "ax1.hist(X[y==1, 0], alpha=.3);\n",
    "ax1.hist(X[y==-1, 0], alpha=.6);\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Проекция на ось $X_1$')\n",
    "ax2.hist(X[y==1, 1], alpha=.3);\n",
    "ax2.hist(X[y==-1, 1], alpha=.6);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(data, eps=0.01):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, eps),\n",
    "                         np.arange(y_min, y_max, eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=17).fit(X, y)\n",
    "\n",
    "\n",
    "xx, yy = get_grid(X, eps=.05)\n",
    "predicted = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='autumn', alpha=0.3)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], marker='x', s=100, cmap='autumn',  linewidth=1.5)\n",
    "plt.scatter(X[y==-1, 0], X[y==-1, 1], marker='o', s=100, cmap='autumn', edgecolors='k',linewidth=1.5)\n",
    "plt.title('Easy task. Decision tree complexifies everything');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_graphviz(tree, out_file='complex_tree.dot', filled=True)\n",
    "# !dot -Tpng 'complex_tree.dot' -o 'complex_tree.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Деревья решений для регрессии (кратко)\n",
    "\n",
    "см. sklearn.DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ансамблирование деревьев. Случайный лес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что если у нас несколько классификаторов (каждый может быть не очень *умным*) ошибающихся на разных объектах\n",
    "Тогда если в качестве предсказания мы будем использовать *моду* мы можем расчитывать на лучшую предсказательную силу.\n",
    "\n",
    "\n",
    "### Идея 1\n",
    "\n",
    "Как получить модели которые ошибаются в разных местах?\n",
    "Давайте брать *тупые* деревья но учить их на **разных подвыборках признаков** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея 2\n",
    "\n",
    "Как получить модели которые ошибаются в разных местах?\n",
    "\n",
    "Давайте брать *тупые* деревья, но учить их на **разных подвыборках объектов** !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат: Случайный лес.\n",
    "\n",
    "sklearn.ensemble RandomForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
