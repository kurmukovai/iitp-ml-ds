{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 5.\n",
    "\n",
    "Ниже приведено решение задачи Walmart TripType prediction.\n",
    "\n",
    "Сначала мы строим простую baseline модель используя алгоритм случайного леса, затем делаем Exploratory Data Analysis, и наконец генерим новый набор признаков и на нем обучаем алгоритм градиентного бустинга.\n",
    "\n",
    "Разберитесь в этом нотбуке: \n",
    "- в чем заключается задача?\n",
    "- какие признаки мы генерим?\n",
    "\n",
    "Существует 3 популярные библиотеки реализующие алгоритм градиентного бустинга:\n",
    "- xgboost https://xgboost.readthedocs.io/en/stable/python/python_intro.html\n",
    "- catboost https://catboost.ai/en/docs/concepts/python-quickstart\n",
    "- lightgbm https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "\n",
    " \n",
    "Решение отправляйте на почту kurmukovai@gmail.com, до полуночи 24 марта, 23:59:59 с темой письма [iitp-intro-ds-2024-ha5-Surname], например [iitp-intro-ds-2024-ha5-Kurmukov]\n",
    "\n",
    "* если вам нужно будет обработать пропуски (missing values) обрабатывайте их любым разумным образом (см. семинар 3)\n",
    "* все 3 библиотеки умеют работать с категориальным переменными \"из коробки\":\n",
    "- https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html\n",
    "- https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html\n",
    "\n",
    "возможно необходимо строковые признаки закодировать в числа 1..n ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# п. 1-5 смотрите ниже\n",
    "\n",
    "\n",
    "Данные без пароля можно скачать тут https://disk.yandex.ru/d/xTXpgXJwiWyYSQ\n",
    "\n",
    "Ниже предложено два baseline решения, изучите их прежде чем переходить к разделам 6-7.\n",
    "\n",
    "\n",
    "# 6. Вопросы про данные\n",
    "\n",
    "В вопросах про данные используйте выборку `train.csv`\n",
    "\n",
    "1. Сколько всего уникальных типов покупок (TripType) в данной задаче?\n",
    "2. В какой день недели совершается больше всего покупок?\n",
    "3. Какой мультиклассовой точности соответствует логлосс 0.997 в данной задаче?\n",
    "4. Опишите качественно самый частый и самый редкий типы покупок в данных: в какой день они происходят, каким категорям товаров соответствуют, какой в них средний размер чека (количество позиций) и т.д.\n",
    "\n",
    "\n",
    "# 7. Compare `xgboost`, `catboost` and `lightgbm`\n",
    "\n",
    "Ваша задача состоит в том чтобы познакомиться с 3 имплементациями градиентного бустинга. Перед вами нет задачи выбить \"наилучшее\" качество. \n",
    "\n",
    "1. Отредактируйте п.4. Разделите данные `train.csv` на тренировочную и тестовую выборки в отношении 8 к 2.\n",
    "\n",
    "2. Запустите модели `xgboost`, `lightgbm` и `catboost` с гиперпараметрами предложенными выше для `catboost`:\n",
    "```\n",
    "ctb_params = {\n",
    "    'depth': 4,\n",
    "    'learning_rate': .3,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'iterations': 100\n",
    "}\n",
    "```\n",
    "остальные гиперпараметры моделей используйте \"по умолчанию\". Сравните качество полученных моделей на \"внутренней\" тестовой выборке из вопроса 1 в терминах логистического лосса и в терминах мультиклассовой точности (multi-class accuracy).\n",
    "\n",
    "3. Перечислите какие гиперпараметры для регуляризации существуют в трех имплементациях градиентного бустинга. Какие из них повторяются, а какие отличаются?\n",
    "\n",
    "4. Повторите п.2, но теперь в качестве гиперпараметров в моделях используйте параметры по-умолчанию из `lightgbm`:\n",
    "`num_leaves`, `learning_rate`, `n_estimators`. Сравните результаты с результатами из вопроса 2. \n",
    "\n",
    "5. (по возможности) сделайте предсказание лучшей из моделей на тестовой выборке (test.csv) для которой у вас нет столбца `TripType`, засабмитьте результат на kaggle и сравните оценки logloss между внутренним и отложенным тестом, прокомментируйте результат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart TripType prediction\n",
    "\n",
    "https://www.kaggle.com/c/walmart-recruiting-trip-type-classification\n",
    "\n",
    ">For this competition, you are tasked with categorizing shopping trip types based on the items that customers purchased. To give a few hypothetical examples of trip types: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes.\n",
    "\n",
    "\n",
    "## Multi-class classification, goal is to predict `type of the trip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c walmart-recruiting-trip-type-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('walmart-recruiting-trip-type-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns description\n",
    "\n",
    "- `TripType` - a categorical id representing the type of shopping trip the customer made. This is the ground truth that you are predicting. TripType_999 is an \"other\" category.\n",
    "- `VisitNumber` - an id corresponding to a single trip by a single customer\n",
    "- `Weekday` - the weekday of the trip\n",
    "- `Upc`- the UPC number of the product purchased\n",
    "- `ScanCount` - the number of the given item that was purchased. A negative value indicates a product return.\n",
    "- `DepartmentDescription` - a high-level description of the item's department\n",
    "- `FinelineNumber` - a more refined category for each of the products, created by Walmart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TripType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand the task.\n",
    "\n",
    "> Every single ml task has its features, and there are no universal solutions, only generally working principles.\n",
    "\n",
    "We need to predict type of the visit: `TripType`. Every row of the data table contains information about a single product not a visit, threfore we need to combine the information about the visit from all the purchases. \n",
    "\n",
    "![](trip-type.png)\n",
    "\n",
    "Unfortunately, we do not have the information about customers, so we do not know if some of the Visits were performed by the same person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a simple baseline model\n",
    "\n",
    "- Count number of purchases in a Visit\n",
    "- Create binary column `is_weekend`\n",
    "- Drop all the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(day):\n",
    "    return int(day in ['Saturday', 'Sunday'])\n",
    "\n",
    "df_train = train.copy()\n",
    "\n",
    "# Generate is_weekend\n",
    "df_train['is_weekend'] = df_train.Weekday.apply(is_weekend)\n",
    "\n",
    "# Generate n_products\n",
    "gp_n_products = df_train.groupby('VisitNumber')['ScanCount'].count()\n",
    "df_train['n_products'] = df_train.VisitNumber.map(gp_n_products)\n",
    "\n",
    "# drop duplicated Visit numbers\n",
    "df_train = df_train.drop_duplicates(subset=['VisitNumber']).reset_index(drop=True)\n",
    "\n",
    "# drop all columns except `is_weekend`, `n_products` and `TripType`\n",
    "df_train = df_train.drop(['VisitNumber', 'Weekday', 'Upc', 'ScanCount',\n",
    "                          'DepartmentDescription', 'FinelineNumber'], axis=1)\n",
    "\n",
    "# Encode TripType so unique values are from 0 to (m-1), where m is number of classes\n",
    "encoder = LabelEncoder().fit(df_train['TripType'])\n",
    "df_train['TripType_encoded'] =  encoder.transform(df_train['TripType'])\n",
    "df_train = df_train.drop('TripType', axis=1)\n",
    "\n",
    "# Create separate variables X and y\n",
    "X = df_train.drop('TripType_encoded', axis=1)\n",
    "y = df_train.TripType_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test, use parameter `stratify=y`\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=774, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a RandomForest model with default hyperparameters\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For this tash we are using a log loss $$- y \\log p - (1-y)\\log(1-p)$$\n",
    "in the following multi class form:\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N \\log {\\left(\\frac{e^{a_{it_i}}}{\\sum_{j=0}^{M-1} e^{a_{ij}}}\\right)}$$\n",
    "\n",
    "$t \\in \\{0\\ldots M-1\\}$, $M$ is number of classes, $N$ is number of objects. Numerator is $a_{it_{i}}$ = \\[ unnormalized probability of an $i$'th object to be assigned to the right class $t_i$\\], thus:\n",
    "\n",
    "$$p_{it_i} = \\frac{e^{a_{it_i}}}{\\sum_{j=0}^{M-1} e^{a_{ij}}}$$\n",
    "\n",
    "see for example https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y_test and compute multi-class log loss of your prediction\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to get an intuition on whether it is a good a bad prediction compute an accuracy of your model\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember this is a 38 class classification, count predicted classes\n",
    "\n",
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts of a TripType on a train set\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with the constant prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a log loss and an accuracy of a constant prediction (predict most frequent TripType)\n",
    "\n",
    "y_dummy = [5]*19135\n",
    "print(accuracy_score(y_test, y_dummy))\n",
    "\n",
    "y_dummy_proba = np.zeros((19135, 38))\n",
    "y_dummy_proba[:, 5] = 1\n",
    "print(log_loss(y_test, y_dummy_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions on the baseline\n",
    "1. Even with this simple features and relatively stupid predictions we are better than a constant prediction.\n",
    "2. Classifier mostly predicts frequent classes.\n",
    "3. Some frequent classes are predicted and some are not. This may be due to the fact that predicted\n",
    "classes are better described by the generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deeper look on the features.\n",
    "\n",
    "### Columns description\n",
    "\n",
    "- `TripType` - a categorical id representing the type of shopping trip the customer made. This is the ground truth that you are predicting. TripType_999 is an \"other\" category.\n",
    "- `VisitNumber` - an id corresponding to a single trip by a single customer\n",
    "- `Weekday` - the weekday of the trip\n",
    "- `Upc`- the UPC number of the product purchased\n",
    "- `ScanCount` - the number of the given item that was purchased. A negative value indicates a product return.\n",
    "- `DepartmentDescription` - a high-level description of the item's department\n",
    "- `FinelineNumber` - a more refined category for each of the products, created by Walmart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features, except `ScanCount` are categorical, a negative value of a `ScanCount` indicates a product return.\n",
    "\n",
    "### 3.1 VisitNumber \n",
    "is an indicator of a visit, we need it to aggregate different purchases,\n",
    "but the number itself is not important it is just an index. Let's have a quick look on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.VisitNumber.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train.VisitNumber.value_counts(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.VisitNumber.value_counts().value_counts().sort_index()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "- More than half of all Visit consists of 4 or less purchases\n",
    "- 90% of Visits consist of 17 or less purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many visits are on different weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "train.Weekday.value_counts().reindex(weekdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have more returns on some weekdays?\n",
    "\n",
    "train['IsReturn'] = train['ScanCount'].apply(lambda x: x < 0)\n",
    "train.groupby(['Weekday'])['IsReturn'].sum().sort_values().reindex(weekdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there only `-1` returns?\n",
    "\n",
    "train[train.IsReturn].ScanCount.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 DepartmentDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most popular Departments? (total sum over ScanCount)\n",
    "\n",
    "train.groupby('DepartmentDescription').ScanCount.sum().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does most popular DepartmentDescription differ for different TripTypes?\n",
    "# > Allows to more or less deanonymize `TripType`.\n",
    "\n",
    "gp = train.groupby('TripType')['DepartmentDescription'].value_counts().reset_index(name='Count')\n",
    "gp2 = gp.groupby(['TripType'])[['Count','DepartmentDescription']]\\\n",
    "                                                           .apply(pd.DataFrame.nlargest, n=3, columns=['Count'])\\\n",
    "                                                           .reset_index()\\\n",
    "                                                           .drop('level_1', axis=1)\n",
    "\n",
    "gp2[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From our baseline, recall most popular TripTypes, let's deanonymize them.\n",
    "# Select subset of the previous table with TripType in [40, 39, 37, 38, 25]\n",
    "\n",
    "gp2[gp2.TripType.isin([40, 39, 37, 38, 25])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the DepartmentDescription with most returns?\n",
    "\n",
    "train.groupby('DepartmentDescription')['IsReturn'].sum().sort_values(ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 FinelineNumber\n",
    "\n",
    "> according to the data description `FinelineNumber` is just a more detailed `DepartmentDescription`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a crosstab between DepartmentDescription and FinelineNumber,\n",
    "# for most popular DepartmentDescription (total ScanCount > 20_000)\n",
    "# and most popular FinelineNumber (total ScanCount > 2000)\n",
    "\n",
    "\n",
    "popular_dd = train.groupby('DepartmentDescription').ScanCount.sum().sort_values(ascending=False)[:10].index\n",
    "sub = train[train.DepartmentDescription.isin(popular_dd)]\n",
    "sub = sub[sub.FinelineNumber.isin(sub.FinelineNumber.value_counts().iloc[:15].index)]\n",
    "\n",
    "tab = pd.crosstab(sub.FinelineNumber, sub.DepartmentDescription)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Upc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Upc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions on EDA\n",
    "\n",
    " - Visits are mostly consist of small number of products\n",
    " - Large visits are on weekends\n",
    " - `TripType` depends on `DepartmentDescription` and `FinelineNumber`\n",
    " - `Upc` is something like a bar code, could be usefull but contains almost 100k unique values\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "# test = pd.read_csv('walmart-recruiting-trip-type-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_nth(x, n=0):\n",
    "    try:\n",
    "        return x[n]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def return_nth_val(x, n=0):\n",
    "    try:\n",
    "        return x[n]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def is_weekend(x):\n",
    "    return int(x in ['Sunday', 'Saturday'])\n",
    "\n",
    "def has_return(x):\n",
    "    return int(any(_x < 0 for _x in x))\n",
    "\n",
    "def sum_return(x):\n",
    "    return np.sum([_x for _x in x if _x < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Most frequent DepartmentDescription in the purchase, second most frequent, third, fourth.\n",
    "    f = lambda grp: grp.value_counts().nlargest(6)\n",
    "    x = df.groupby('VisitNumber')['DepartmentDescription'].apply(f).reset_index()\n",
    "    gp = x.groupby('VisitNumber')['level_1'].unique()\n",
    "\n",
    "    df['PopularCategory'] = df.VisitNumber.map(gp)\n",
    "    for i in range(4):\n",
    "        df[f'Category_{i}'] = df['PopularCategory'].apply(return_nth, args=[i])\n",
    "\n",
    "    # 2. Same for FinelineNumber, but 6\n",
    "    x = df.groupby('VisitNumber')['FinelineNumber'].apply(f).reset_index()\n",
    "    gp = x.groupby('VisitNumber')['level_1'].unique()\n",
    "\n",
    "    df['PopularFineline'] = df.VisitNumber.map(gp)\n",
    "    for i in range(6):\n",
    "        df[f'Fineline_{i}'] = df['PopularFineline'].apply(return_nth, args=[i]).astype(object)\n",
    "\n",
    "    # 3. Count number of unique DepartmentDescription in the purchase\n",
    "    gp = df.groupby('VisitNumber')['DepartmentDescription'].nunique()\n",
    "    df['#Unique_Department'] = df.VisitNumber.map(gp)\n",
    "\n",
    "    # 4. Count number of unique FinelineNumber in the purchase.\n",
    "    gp = df.groupby('VisitNumber')['FinelineNumber'].nunique()\n",
    "    df['#Unique_Fineline'] = df.VisitNumber.map(gp)\n",
    "\n",
    "    # 5. Count ScanCount (number of unique Upc in the purchase)\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['ScanCount'].count()\n",
    "    df['#UniqueScanCount'] = df['VisitNumber'].map(gp)   \n",
    "\n",
    "    # 6. Sum ScanCount\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['ScanCount'].sum()\n",
    "    df['TotalScanCount'] = df['VisitNumber'].map(gp)\n",
    "\n",
    "    # 7. Is weekend\n",
    "\n",
    "    df['is_Weekend'] = df['Weekday'].apply(is_weekend).astype(object)\n",
    "\n",
    "    # 8. Sum returns\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['ScanCount'].apply(sum_return)\n",
    "    df['total_return'] = df.VisitNumber.map(gp)\n",
    "    df['total_return'].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop old and intermediate features\n",
    "\n",
    "    df = df.drop(['Upc', 'ScanCount', 'DepartmentDescription',\n",
    "                  'FinelineNumber', 'PopularFineline', 'PopularCategory'], axis=1)\n",
    "\n",
    "    # Drop duplicated rows (with the same VisitNumber)\n",
    "\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this cell takes about 10 minutes\n",
    "df = generate_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(df.columns):\n",
    "    print(i-2, col, df[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col.startswith('Category'):\n",
    "        df[col]=df[col].apply(str)\n",
    "    if col.startswith('Fineline'):\n",
    "        df[col]=df[col].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna('NaN', inplace=True)\n",
    "x_train, x_eval = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "cat_features = [*list(range(10)), 15]\n",
    "\n",
    "data_train = Pool(x_train.drop(['TripType', 'VisitNumber'], axis=1), \n",
    "                  label=x_train.TripType,\n",
    "                  cat_features=cat_features)\n",
    "\n",
    "data_eval = Pool(x_eval.drop(['TripType', 'VisitNumber'], axis=1), \n",
    "                 label=x_eval.TripType,\n",
    "                 cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_params = {\n",
    "    'depth': 4,\n",
    "    'learning_rate': .3,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'verbose': 1,\n",
    "    'thread_count': 12,\n",
    "    'iterations': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**ctb_params)\n",
    "model.fit(X = data_train, silent=False, eval_set=data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_model('catboost_depth=4_lr=.3_l2=3')\n",
    "# model = CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
